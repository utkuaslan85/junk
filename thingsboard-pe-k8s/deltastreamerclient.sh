spark-submit \
    --conf spark.driver.extraJavaOptions="-Divy.cache.dir=/tmp -Divy.home=/tmp" \
    --conf spark.serializer=org.apache.spark.serializer.KryoSerializer \
    --conf spark.hadoop.fs.s3a.impl=org.apache.hadoop.fs.s3a.S3AFileSystem \
    --conf spark.hadoop.fs.s3a.access.key=JBOU35ZHUL7ILTY0JI3Y \
    --conf spark.hadoop.fs.s3a.secret.key=DSG4BKmyeDKQ6CWfrATgZnttangEkUTxsNCU3M4X \
    --conf spark.hadoop.fs.s3a.endpoint=http://10.6.28.200:80 \
    --conf spark.hadoop.fs.s3a.path.style.access=true \
    --conf spark.hadoop.fs.s3a.fast.upload=true \
    --conf spark.hadoop.fs.s3a.connection.ssl.enabled=false \
    --conf spark.hadoop.com.amazonaws.services.s3.enableV4=true \
    --conf spark.hadoop.fs.s3a.aws.credentials.provider=org.apache.hadoop.fs.s3a.SimpleAWSCredentialsProvider \
    --conf spark.hadoop.com.amazonaws.auth.InstanceProfileCredentialsProvider=true \
    --conf spark.hadoop.com.amazonaws.services.s3.enableV4=true \
    --conf spark.hadoop.fs.s3a.committer.name=directory \
    --conf spark.sql.sources.commitProtocolClass=org.apache.spark.internal.io.cloud.PathOutputCommitProtocol \
    --conf spark.sql.parquet.output.committer.class=org.apache.spark.internal.io.cloud.BindingParquetOutputCommitter \
    --conf spark.sql.catalog.spark_catalog=org.apache.spark.sql.hudi.catalog.HoodieCatalog \
    --conf spark.sql.hive.convertMetastoreParquet=false \
    --conf spark.hadoop.fs.s3a.fast.upload.buffer=array \
    --conf spark.submit.deployMode=client \
    --class org.apache.hudi.utilities.deltastreamer.HoodieDeltaStreamer \
    --master local[*] \
    --packages com.amazonaws:aws-java-sdk-bundle:1.12.162 \
    http://10.6.28.200:80/data/jars/hudi-utilities-slim-bundle_2.12-0.12.0.jar \
        --table-type COPY_ON_WRITE \
        --source-ordering-field "ts" \
        --hoodie-conf hoodie.upsert.shuffle.parallelism=10 \
        --hoodie-conf hoodie.insert.shuffle.parallelism=10 \
        --hoodie-conf hoodie.delete.shuffle.parallelism=10 \
        --hoodie-conf hoodie.bulkinsert.shuffle.parallelism=10 \
        --hoodie-conf hoodie.table.name=tb_kafka_avro \
        --hoodie-conf hoodie.parquet.compression.codec=snappy \
        --hoodie-conf hoodie.cleaner.commits.retained=10 \
        --hoodie-conf hoodie.clean.automatic=true \
        --hoodie-conf hoodie.clean.async=true \
        --hoodie-conf hoodie.client.heartbeat.tolerable.misses=10 \
        --hoodie-conf hoodie.datasource.write.hive_style_partitioning=true \
        --hoodie-conf hoodie.datasource.write.recordkey.field=ts \
        --hoodie-conf hoodie.datasource.write.partitionpath.field=data_key \
        --hoodie-conf hoodie.datasource.write.precombine.field=ts \
        --hoodie-conf hoodie.datasource.write.drop.partition.columns=true \
        --hoodie-conf hoodie.deltastreamer.schemaprovider.registry.url=http://10.105.144.163:8085/subjects/tb-kafka-avro8-value/versions/latest \
        --hoodie-conf hoodie.deltastreamer.source.kafka.topic=tb-kafka-avro8 \
        --hoodie-conf hoodie.datasource.hive_sync.user=root \
        --hoodie-conf hoodie.datasource.hive_sync.password=mypass \
        --hoodie-conf hoodie.datasource.hive_sync.table=tb_kafka_avro \
        --hoodie-conf hoodie.datasource.hive_sync.metastore.uris=thrift://10.111.246.27:9083 \
        --hoodie-conf hoodie.datasource.hive_sync.mode=hms \
        --hoodie-conf hoodie.datasource.hive_sync.use_jdbc=false \
        --hoodie-conf hoodie.datasource.hive_sync.base-path=s3a://data/lake/ \
        --hoodie-conf hoodie.datasource.hive_sync.skip_ro_suffix=true \
        --hoodie-conf hoodie.datasource.hive_sync.database=default \
        --hoodie-conf hoodie.datasource.hive_sync.partition_fields=data_key \
        --hoodie-conf hoodie.datasource.hive_sync.enable=true \
        --hoodie-conf bootstrap.servers=10.102.117.121:9092 \
        --hoodie-conf auto.offset.reset=earliest \
        --hoodie-conf schema.registry.url=http://10.105.144.163:8085 \
        --source-class org.apache.hudi.utilities.sources.AvroKafkaSource \
        --target-base-path s3a://data/lake/tb_kafka_avro \
        --target-table tb_kafka_avro \
        --enable-hive-sync \
        --schemaprovider-class org.apache.hudi.utilities.schema.SchemaRegistryProvider \
        --continuous
